---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}


<!-- æ®µè½æ³¨é‡Š -->
[//]: æ®µè½æ³¨é‡Š
<!-- æ–‡å­—é¢œè‰²$\color{RoyalBlue} {Editorial Board}$  RoyalBlue, DarkBlue, Navy-->
<!-- æ–‡å­—é¢œè‰²<font color=lightgrey>Editorial Board</font> -->
<!--  æ–‡å­—é¢œè‰² <font color=Blue>Editorial Board</font>  <font color=Tomato>accepted</font> -->
<!--  <span style="color:rgb(17,85,160)">Professional Activities</span> -->
<!--  
<span style="color:rgb(17,85,160)">Biography</span>
# ğŸ‘¦ğŸ”ŠğŸ“¢ğŸ“£
-->



<!-- ä¸­è‹±å…¨å±€æœ‰æ•ˆ -->
ğŸ“¢æ‹›æ”¶2026å¹´å…¥å­¦ç ”ç©¶ç”Ÿï¼Œæ¬¢è¿è”ç³»ã€‚<br>





<!--    ğŸ‘¦ğŸ”ŠğŸ“¢ğŸ“£ Biography      --> 
<!-- ä¸­æ–‡å— -->
<div class="i18n zh">
<span class='anchor' id='about-me'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">ä¸ªäººç®€ä»‹</h1>

å´ç§‘å›(<a href="http://faculty.hust.edu.cn/WuKejun/en/index.htm" target="_blank">æ•™å¸ˆä¸»é¡µ</a>) ï¼Œåšå£«ï¼ŒIEEEé«˜çº§ä¼šå‘˜ï¼Œäº2024å¹´åŠ å…¥åä¸­ç§‘æŠ€å¤§å­¦ç”µå­ä¿¡æ¯ä¸é€šä¿¡å­¦é™¢ä»»è®²å¸ˆ/åŠ©ç†æ•™æˆã€‚åœ¨æ­¤ä¹‹å‰ï¼Œä»–äºæ–°åŠ å¡å—æ´‹ç†å·¥å¤§å­¦ç”µæ°”ç”µå­å·¥ç¨‹å­¦é™¢ä»äº‹åšå£«åç ”ç©¶ï¼ˆä¸Yap Kim-Huiã€<a href="http://faculty.hust.edu.cn/WuKejun/en/index.htm" target="_blank">å‘¨ç«‹åŸ¹</a>æ•™æˆ (IEEE Fellow)ï¼‰ã€‚ä»–è·å¾—äº†åä¸­ç§‘æŠ€å¤§å­¦åšå£«å­¦ä½ï¼ˆå¯¼å¸ˆ æ¨é“€ æ•™æˆï¼‰ã€å“ˆå°”æ»¨å·¥ç¨‹å¤§å­¦ç¡•å£«å­¦ä½ï¼ˆå¯¼å¸ˆ <a href="http://cisse.hrbeu.edu.cn/info/1088/3385.htm" target="_blank">è”¡æˆæ¶›</a> æ•™æˆï¼‰ä»¥åŠä¸Šæµ·å¤§å­¦å­¦å£«å­¦ä½ã€‚ä»–çš„ç ”ç©¶å…´è¶£ä¸»è¦åŒ…æ‹¬å¤šæ¨¡æ€å¤§æ¨¡å‹ã€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€é«˜ç»´è§†è§‰å’Œè§†é¢‘å‹ç¼©ä¸ç†è§£ç­‰é¢†åŸŸï¼Œè¿‘5å¹´åœ¨IEEE T-MMã€IEEE T-CSVTã€NeurIPSã€ACM Multimediaç­‰é«˜æ°´å¹³æœŸåˆŠã€ä¼šè®®å‘è¡¨è®ºæ–‡50ä½™ç¯‡ã€‚æ‹…ä»»IEEE OJSPã€ASOCã€JRTIPã€JVCIç­‰æƒå¨æœŸåˆŠçš„å‰¯ä¸»ç¼–/ç¼–å§”/å®¢åº§ç¼–è¾‘ç­‰ï¼Œå¹¶åœ¨å›½é™…ä¼šè®®AAAI 2026ã€AIGC 2025ã€IJCNN 2025ã€IEEE ICASSP2024ã€ISCAS2024å’ŒMMSP2023ä¸­æ‹…ä»»ç¨‹åºå§”å‘˜ä¼š/é¢†åŸŸä¸»å¸­/ä¸“é¢˜ä¸»å¸­ç­‰ã€‚<br>
ğŸš€ &nbsp; æ‹›æ”¶å…´è¶£ä»äº‹ä»¥ä¸Šç ”ç©¶é¢†åŸŸçš„å®ä¹ ç”Ÿ/è®¿é—®å­¦ç”Ÿ/ç ”ç©¶ç”Ÿ, æ¬¢è¿çº¿ä¸‹æˆ–è¿œç¨‹å®ä¹ /å­¦ä¹ ï¼Œæä¾›è®¡ç®—èµ„æºå’Œæ´¥è´´ã€‚

</div>


<!-- è‹±æ–‡å— -->
<div class="i18n en">
<span class='anchor' id='about-me'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">Biography</h1>

Dr. Kejun Wu (<a href="http://faculty.hust.edu.cn/WuKejun/en/index.htm" target="_blank">Faculty HomePage</a>) is currently a Lecturer/Assistant Professor at School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China. Before that, he worked as a Research Fellow researcher at School of Electrical and Electronic Engineering, Nanyang Technological University (work with Prof. Yap Kim-Hui and <a href="http://faculty.hust.edu.cn/WuKejun/en/index.htm" target="_blank">Chau Lap-Pui</a> (IEEE Fellow)). 
He obtained his Ph.D. degree from Huazhong University of Science and Technology (supervised by You Yang), Master degree from Harbin Engineering University (supervised by <a href="http://cisse.hrbeu.edu.cn/info/1088/3385.htm" target="_blank">Chengtao Cai</a>) Bachelor degree from Shanghai University.
His research interest includes Multimodal Large Language Models, Generative AI, High-dimensional Vision, and Learned Video Compression. He has over 50 publications in top-tier venues in the past 5 years. He is a Technical Committee Affiliate of IEEE SPS IVMSP and IEEE SPS MMSP.
He serves as an Associate Editor / Editorial Board Member / Executive Guest Editor of IEEE OJSP, ASOC, JVCI, and JRTIP, etc., an Area Chair / Session Chair / Program Committee in AAAI 2026, AIGC 2025, IJCNN 2025, IEEE ICASSP 2024,ISCAS 2024, and MMSP2023. He is an IEEE Senior Member.<br>
ğŸš€ &nbsp; Recruiting intern/ visiting/ Master students interested in the aforementioned research areas. On-site, remote, or hybrid interning/learning are welcome, Computing resources and allowance provided.
<!--  Contact via <kjwu@hust.edu.cn>  -->

</div>


 
 

<!-- æ™®é€šç‰ˆã€è¯¦ç»†ç‰ˆ
Dr. Kejun Wu ([Faculty HomePage](http://faculty.hust.edu.cn/WuKejun/en/index.htm)) is currently a Lecturer/Assistant Professor at School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China. Before that, he worked as a Research Fellow researcher from 2022 to 2024 at School of Electrical and Electronic Engineering, Nanyang Technological University (work with Prof. Yap Kim-Hui and Chau Lap-Puiå‘¨ç«‹åŸ¹). 
He obtained his Ph.D. degree from Huazhong University of Science and Technology supervised by You Yang (æ¨é“€), Master degree from Harbin Engineering University supervised by Chengtao Cai (è”¡æˆæ¶›), and Bachelor degree from Shanghai University.
His research interest includes Multimodal Large Language Models, Generative Signal/Image Processing, and High-dimensional Vision, Learned Video Compression. He has published more than 40 papers in the past 3 years. He serves as an Associate Editor / Editorial Board Member / Executive Guest Editor of ASOC, OJSP, JVCI, and JRTIP, etc., an Area Chair / Session Chair / Program Committee Member in IJCNN 2025, AIGC 2025, IEEE ICASSP2024, ISCAS2024, and MMSP2023. He is an IEEE Senior Member.

Dr. Kejun Wu is a Lecturer at the School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China. He obtained his Ph.D. degree from Huazhong University of Science and Technology supervised by You Yang (æ¨é“€), and Master degree from Harbin Engineering University supervised by Chengtao Cai (è”¡æˆæ¶›).
He was a visiting Phd student at Nanyang Technological University, Singapore from 2021 to 2022 supervised by Prof. Chau Lap-Pui (å‘¨ç«‹åŸ¹). From 2022 to 2024, Dr. Wu worked as a Research Fellow researcher at the School of Electrical and Electronic Engineering, Nanyang Technological University (work with Prof. Yap Kim-Hui and Chau Lap-Pui). His research interests include Generative AI, Large Language Models and their applications. He has published over 40 papers at top venues including IEEE TPAMI / TMM / TCSVT, NeurIPS, ACM MM, etc. His research achievements have significant academic impact and was rewarded the National Postdoctoral Innovation and Entrepreneurship Competition (ranked first), â€œChunhui Cupâ€ Innovation and Entrepreneurship Competition for Chinese Overseas Students (ranked first), China International College Studentsâ€™ Innovation Competition and other awards. He has taught the undergraduate students of Oxford University computational photography internship course in 2022 and received Letter of Appreciation from Oxford University; He was granted the Silver Award in the IET Excellence and Innovation Awards and IET Impact in Society Awards Shortlist by IET (Institute of Engineering and Technology); He has served as Young Researcher of the Decision-making Consulting Expert Team of the China Association for Science and Technology. He has served as Session Chairs/Organizers in international conferences IEEE ICASSP 2024, IEEE ISCAS 2024, IEEE MMSP 2023, Associate Editor in MTAP journal, and Lead Guest Editor in JVCI (Journal of Visual Communication and Image Representation). He is an IEEE Senior Member.

My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).
-->




<!--        ğŸ“¢ Professional Services     --> 
<!-- ä¸­æ–‡å— -->
<div class="i18n zh">
<span class='anchor' id='professional-activities'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">å­¦æœ¯æœåŠ¡</h1>

<div style="height: 400px; overflow-y: auto; border: 1px solid #ccc;">
  <ul>
    <li>ç¼–å§”, å›½é™…æœŸåˆŠApplied Soft Computing (JCR Q1åŒº) </li>  
    <li>å‰¯ä¸»ç¼–, å›½é™…æœŸåˆŠIEEE Open Journal of Signal Processing </li>  
    <li>å®¢åº§ç¼–è¾‘, å›½é™…æœŸåˆŠJournal of Real-Time Image Processing (JCR Q2åŒº) </li>  
    <li>å®¢åº§ç¼–è¾‘, å›½é™…æœŸåˆŠJournal of Visual Communication and Image Representation (JCR Q2åŒº) </li>  
    <li>ç¨‹åºå§”å‘˜ä¼š, å›½é™…ä¼šè®®AAAI 2026, æ–°åŠ å¡ </li>  
    <li>ç¨‹åºå§”å‘˜ä¼š, å›½é™…ä¼šè®®AIGC 2025, æ­å· </li>
    <li>é¢†åŸŸä¸»å¸­, å›½é™…ä¼šè®®IJCNN 2025, æ„å¤§åˆ© </li>  
    <li>ä¸“é¢˜ä¸»å¸­, å›½é™…ä¼šè®®ISCAS 2024, æ–°åŠ å¡ </li>  
    <li>ä¸“é¢˜ä¸»å¸­, å›½é™…ä¼šè®®ICASSP 2024, éŸ©å›½ </li>
    <li>ä¸“é¢˜ä¸»å¸­, å›½é™…ä¼šè®®MMSP2023, æ³•å›½ </li>  
    <li>åº”æ€¥æ€åŠ¿æ„ŸçŸ¥ä¸åº”æ€¥é€šä¿¡æŠ€æœ¯å†³ç­–å’¨è¯¢ä¸“å®¶å›¢é˜Ÿæˆå‘˜, ä¸­å›½ç§‘å </li>
    <li>UWAæŠ€æœ¯è§„åˆ’ç»„ä¸“å®¶, ä¸–ç•Œè¶…é«˜æ¸…è§†é¢‘äº§ä¸šè”ç›ŸUWA </li>  
    <li>Technical Committee Affiliate, Image, Video, and Multidimensional Signal Processing (IVMSP), IEEE ä¿¡å·å¤„ç†å­¦ä¼š </li>
    <li>Technical Committee Affiliate, Multimedia Signal Processing (MMSP), IEEE ä¿¡å·å¤„ç†å­¦ä¼š </li>  
    <li>å®¡ç¨¿äººï¼šIEEE TMM, IEEE TCSVT, IEEE TBc, Pattern Recognition, Information Fusion, NeurIPS, AAAI, ICASSP, etc. </li>    
  </ul>
</div>

</div>


<!-- è‹±æ–‡å— -->
<div class="i18n en">
<span class='anchor' id='professional-activities'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">Professional Activities</h1>

<div style="height: 400px; overflow-y: auto; border: 1px solid #ccc;">
  <ul>
    <li>Editorial Board, Applied Soft Computing </li>  
    <li>Associate Editor, IEEE Open Journal of Signal Processing </li>  
    <li>Lead Guest Editor, Journal of Real-Time Image Processing </li>  
    <li>Lead Guest Editor, Journal of Visual Communication and Image Representation </li>  
    <li>Program Committee, AAAI 2026, Singapore </li>  
    <li>Program Committee, AIGC 2025, Hangzhou, China </li>
    <li>Area Chair, IJCNN 2025, Italy </li>  
    <li>Session Chair, IEEE ISCAS 2024, Singapore </li>  
    <li>Session Chair, IEEE ICASSP 2024, Korea </li>
    <li>Session Chair, IEEE MMSP 2023, France </li>  
    <li>Member, åº”æ€¥æ€åŠ¿æ„ŸçŸ¥ä¸åº”æ€¥é€šä¿¡æŠ€æœ¯å†³ç­–å’¨è¯¢ä¸“å®¶å›¢é˜Ÿ, ä¸­å›½ç§‘å </li>
    <li>Experts, UWA Technical Planning Expert Group, UHD World Association (UWA, ä¸–ç•Œè¶…é«˜æ¸…è§†é¢‘äº§ä¸šè”ç›Ÿ) </li>  
    <li>Technical Committee Affiliate, Image, Video, and Multidimensional Signal Processing (IVMSP), IEEE Signal Processing Society (SPS) </li>
    <li>Technical Committee Affiliate, Multimedia Signal Processing (MMSP), IEEE SPS </li>  
    <li>Reviewer of IEEE TMM, IEEE TCSVT, IEEE TBc, Pattern Recognition, Information Fusion, NeurIPS, AAAI, ICASSP, etc. </li>  
  </ul>
</div>  

</div>
 






<!--       ğŸ”¥ News      -->


<!-- ä¸­æ–‡å— -->
<div class="i18n zh">
<span class='anchor' id='news'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">æ–°é—»åŠ¨æ€</h1>

<div style="height: 400px; overflow-y: auto; border: 1px solid #ccc;">
  <ul>
    <li>2025.08:&thinsp; æ‹…ä»» AAAI 2026 å¤§ä¼šï¼ˆæ–°åŠ å¡ï¼‰ç¨‹åºå§”å‘˜ä¼šå§”å‘˜</li>  
    <li>2025.07:&thinsp; å—é‚€ä»¥è‹±è¯­æˆè¯¾äº 2025 åä¸­ç§‘æŠ€å¤§å­¦â€œMetaversâ€å›½é™…æš‘æœŸå­¦æ ¡ Discover At HUST</li>    
    <li>2025.07:&thinsp; 2 ç¯‡è®ºæ–‡è¢«ç¬¬ 33 å±Š ACM å›½é™…å¤šåª’ä½“ä¼šè®®ï¼ˆACM Multimedia 2025ï¼Œçˆ±å°”å…°éƒ½æŸæ—ï¼‰å½•ç”¨</li>
    <li>2024.06:&thinsp; åŸºäºç”Ÿæˆå¼ AI çš„å›¾åƒä¿®å¤ç ”ç©¶æ­£åœ¨ IEEE TPAMI å®¡ç¨¿ä¿®æ”¹ä¸­</li>
    <li>2025.06:&thinsp; 1 ç¯‡è®ºæ–‡è¢« IEEE Transactions on Multimedia å½•ç”¨ï¼ˆé€šè®¯ä½œè€…ï¼‰</li>
    <li>2025.06:&thinsp; æ‹…ä»»å›½é™… AI ç”Ÿæˆå†…å®¹å¤§ä¼šï¼ˆAIGC 2025ï¼Œæ­å·ï¼‰ç¨‹åºå§”å‘˜ä¼šå§”å‘˜</li>    
    <li>2024.12:&thinsp; 1 ç¯‡è®ºæ–‡è¢« ACM Transactions on Multimedia Computing, Communications, and Applications å½•ç”¨ï¼ˆJCR Q1ï¼‰</li>
    <li>2024.11:&thinsp; 2 ç¯‡è®ºæ–‡è¢« IEEE Transactions on Multimedia å½•ç”¨ï¼ˆä¸­ç§‘é™¢ä¸€åŒº TOPï¼ŒJCR Q1ï¼‰</li>
    <li>2024.08:&thinsp; 1 ç¯‡è®ºæ–‡è¢« Computer Vision and Image Understanding å½•ç”¨ï¼ˆJCR Q1ï¼‰</li>
    <li>2024.05:&thinsp; å—è˜ä¸º IEEE ISCAS 2024ï¼ˆæ–°åŠ å¡ï¼‰ç‰¹é‚€ä¸“é¢˜ä¸»å¸­</li>
    <li>2024.04:&thinsp; å—è˜ä¸º IEEE ICASSP 2024ï¼ˆéŸ©å›½ï¼‰ç‰¹é‚€ä¸“é¢˜ä¸»å¸­</li>
    <li>2024.01:&thinsp; 2 ç¯‡è®ºæ–‡è¢« Optics Express å’Œ Optics Letters å½•ç”¨</li>
    <li>2023.12:&thinsp; è·ä¸­å›½å›½é™…å¤§å­¦ç”Ÿåˆ›æ–°å¤§èµ›å…¨å›½é‡‘å¥–</li>
    <li>2023.11:&thinsp; è·ä¸­å›½åšå£«ååˆ›æ–°åˆ›ä¸šå¤§èµ›å…¨å›½é“œå¥–</li>
    <li>2023.10:&thinsp; è·æ˜¥æ™–æ¯ä¸­å›½ç•™å­¦äººå‘˜åˆ›æ–°åˆ›ä¸šå¤§èµ›ä¼˜èƒœå¥–</li>
    <li>2023.09:&thinsp; å—è˜ä¸º IEEE MMSP 2023ï¼ˆæ³•å›½ï¼‰ç‰¹é‚€ä¸“é¢˜ä¸»å¸­</li>
    <li>2023.09:&thinsp; 1 ç¯‡è®ºæ–‡è¢« NeurIPS 2023ï¼ˆç¾å›½æ–°å¥¥å°”è‰¯ï¼‰å½•ç”¨</li>
  </ul>
</div>


</div>


<!-- è‹±æ–‡å— -->
<div class="i18n en">
<span class='anchor' id='news'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">News</h1>
  
<div style="height: 400px; overflow-y: auto; border: 1px solid #ccc;">
  <ul>
    <li>2025.08:&thinsp; Serve as Program Committee in AAAI 2026 conference, Singapore </li>  
    <li>2025.07:&thinsp; Invited to lecture (in English) the 2025 Discover At HUST--"Metavers" International Summer School </li>    
    <li>2025.07:&thinsp; 2 papers are accepted by 33rd ACM International Conference on Multimedia (ACM Multimedia 2025), Dublin, Ireland </li>
    <li>2024.06:&thinsp; Research on Gen-AI based image restoration is Under Revision at IEEE TPAMI</li>
    <li>2025.06:&thinsp; 1 paper is accepted by IEEE Transactions on Multimedia (Corresponding Author)</li>
    <li>2025.06:&thinsp; Serve as Program Committee for International Conference on AI-Generated Content (AIGC 2025), Hangzhou</li>    
    <li>2024.12:&thinsp; 1 paper is accepted by ACM Transactions on Multim. Comput. Commun. Appl. (JCR Q1)</li>
    <li>2024.11:&thinsp; 2 papers are accepted by IEEE Transactions on Multimedia (ä¸­ç§‘é™¢ä¸€åŒºTOPï¼ŒJCR Q1)</li>
    <li>2024.08:&thinsp; 1 paper is accepted by Computer Vision and Image Understanding(JCR Q1)</li>
    <li>2024.05:&thinsp; I am appointed as Special Session Chair by IEEE ISCAS 2024, Singapore</li>
    <li>2024.04:&thinsp; I am appointed as Special Session Chair by IEEE ICASSP 2024, Korea</li>
    <li>2024.01:&thinsp; 2 papers are accepted by Optics Express and Optics Letters</li>
    <li>2023.12:&thinsp; I am awarded the å…¨å›½é‡‘å¥– in ä¸­å›½å›½é™…å¤§å­¦ç”Ÿåˆ›æ–°å¤§èµ›</li>
    <li>2023.11:&thinsp; I am awarded the å…¨å›½é“œå¥– in ä¸­å›½åšå£«ååˆ›æ–°åˆ›ä¸šå¤§èµ›</li>
    <li>2023.10:&thinsp; I am awarded the ä¼˜èƒœå¥– in æ˜¥æ™–æ¯ä¸­å›½ç•™å­¦äººå‘˜åˆ›æ–°åˆ›ä¸šå¤§èµ›</li>
    <li>2023.09:&thinsp; I am appointed as Special Session Chair by IEEE MMSP 2023, France</li>
    <li>2023.09:&thinsp; 1 paper has been accepted by NeurIPS 2023, New Orleans, USA</li>
  </ul>
</div>

</div>


 



<!--     # Selected Publications ğŸ“     -->

<!-- ä¸­æ–‡å— -->
<div class="i18n zh">
<span class='anchor' id='publications'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">ç²¾é€‰å‘è¡¨</h1>

<div style="height: 400px; overflow-y: auto; border: 1px solid #ccc;">
  <ul>
    
<!-- æ®µè½æ³¨é‡Š  
    <li>&thinsp;Tianyi Liu, <b>Kejun Wu</b>, Yi Wang, Wenyang Liu, Kim-Hui Yap, Lap-Pui Chau, "BitsCV: Restoration of Artifacts Decoded in Bitstream-Corrupted Videos," in <b>IEEE Transactions on Pattern Analysis and Machine Intelligence</b></li>
    <li>&thinsp;Wenyang Liu, Tianyi Liu, Chen Cai, Jianjun Gao, Kejun Wu, Kim-Hui Yap, "AsTaSR: Adaptive Superpixel Token Aggregation for Lightweight Image Super-Resolution," AAAI 2026
-->

  <li>&thinsp;T. Liu, <b>K. Wu</b>, C. Cai, Y. Wang, K. Yap, L. Chau, "Towards Blind Bitstream-corrupted Video Recovery: A Visual Foundation Model-driven Framework," in <b>ACM MM 2025</b> (CCF-A)</li>
  <li>&thinsp;C. Cai, T. Liu, J. Gao, W. Liu, <b>K. Wu</b><font color=Blue>*</font>, R. Wang, Y. Wang, S. Liew, "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Grounded Situation Recognition," in <b>ACM MM 2025</b> (Corresponding Author, CCF-A) </li>    
  <li>&thinsp;T. Liu, <b>K. Wu</b><font color=Blue>*</font>, Y. Wang, W. Liu, K. Yap, and L. Chau, â€œBitstream corrupted Video Recovery: A Novel Benchmark Dataset and Method,â€ <b>NeurIPS 2023</b>. (Equal Contribution, CCF-A)</li>
  <li>&thinsp;<b>K. Wu</b>, Z. Li, Y. Yang, Q. Liu, and X. Zhang, â€œEnd-to-end Deep Video Compression Based on Hierarchical Temporal Context Learning,â€ in <b>IEEE Transactions on Multimedia</b>, 2025. (ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;W. Liu, C. Cai, J. Gao, <b>K. Wu</b><font color=Blue>*</font>, Y. Wang, K. Yap, and L. Chau, â€œPromptSR: Cascade Prompting for Lightweight Image Super-Resolution,â€ in <b>IEEE Transactions on Multimedia</b>, 2025. (Corresponding Author, ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;<b>K. Wu</b>, Y. Yang, G. Jiang, and X. Zhang, â€œHierarchical Independent Coding Scheme for Varifocal Multiview Images based on Angular-focal Joint Prediction,â€ <b>IEEE Transactions on Multimedia</b>, 26:2993-3006, 2024. (ä¸­ç§‘é™¢1åŒºTOP) $\color{Tomato} {ESI &thinsp;Highly &thinsp;Cited &thinsp;Papers}$</li>
  <li>&thinsp;W. Liu, <b>K. Wu</b><font color=Blue>*</font>, T. Liu, Y. Wang, K. Yap, and L. Chau, â€œByteNet: Rethinking Multimedia File Fragment Classification through Visual Perspectives,â€ in <b>IEEE Transactions on Multimedia</b>, 2024. (Corresponding Author, ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, and X. Zhang, â€œFocal Stack Image Compression Based on Basis-Quadtree Representation,â€ in <b>IEEE Transactions on Multimedia</b>, 25:3975-3988, 2023. (ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, Y. Yang, and X. Zhang, â€œGaussian-Wiener Representation and Hierarchical Coding Scheme for Focal Stack Images,â€ in <b>IEEE Transactions on Circuits and Systems for Video Technology</b>, 32(2):523-537, 2022. (ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;<b>K. Wu</b>, Z. Li, Y. Yang, and Q. Liu, â€œDeep Video Compression based on Long-range Temporal Context Learning,â€ in Computer Vision and Image Understanding, 248(2024): 104127.</li>  
  <li>&thinsp;X. Yu, <b>K. Wu</b><font color=Blue>*</font>, Y. Yang, and Q. Liu, â€œWaRENet: A Novel Urban Waterlogging Risk Evaluation Network,â€ in ACM Transactions on Multimedia Computing, Communications, and Applications, 2024, 20(7):1â€“28. (Equal Contribution)</li>
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, K. Yap, and Y. Yang, â€œMultifocal Multiview Imaging and Data Compression based on Angular-Focal-Spatial Representation,â€ in Optics Letters, 2024.</li>
  <li>&thinsp;J. Gao, K. Yap, K. Wu, D. Phan, and K. Garg, "Contextual Human Object Interaction Understanding from Pre-Trained Large Language Model," ICASSP 2024, Seoul, Korea</li>
  <li>&thinsp;C. Cai, R. Zhang, J. Gao, K. Wu, K. Yap, Y. Wang, "Temporal Sentence Grounding with Temporally Global Textual Knowledge," ICME 2024, Niagra Falls, Canada</li>
<!-- æ®µè½æ³¨é‡Š    
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, K. Yap, and Y. Yang, â€œHigh Dimensional Optical Data Varifocal Multiview Imaging, Compression and Evaluation,â€ in Optics Express, 2023.</li>
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, Y. Wang, and Y. Yang, â€œEnd-to-end Varifocal Multiview Images Coding Framework from Data Acquisition End to Vision Application End,â€ in Optics Express, 31(7): 11659-11679, 2023.</li>  
-->
  </ul>
</div>


</div>


<!-- è‹±æ–‡å— -->
<div class="i18n en">
<span class='anchor' id='publications'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">Selected Publications</h1>

<div style="height: 400px; overflow-y: auto; border: 1px solid #ccc;">
  <ul>
    
<!-- æ®µè½æ³¨é‡Š  
    <li>&thinsp;Tianyi Liu, <b>Kejun Wu</b>, Yi Wang, Wenyang Liu, Kim-Hui Yap, Lap-Pui Chau, "BitsCV: Restoration of Artifacts Decoded in Bitstream-Corrupted Videos," in <b>IEEE Transactions on Pattern Analysis and Machine Intelligence</b></li>
    <li>&thinsp;Wenyang Liu, Tianyi Liu, Chen Cai, Jianjun Gao, Kejun Wu, Kim-Hui Yap, "AsTaSR: Adaptive Superpixel Token Aggregation for Lightweight Image Super-Resolution," AAAI 2026
-->

  <li>&thinsp;T. Liu, <b>K. Wu</b>, C. Cai, Y. Wang, K. Yap, L. Chau, "Towards Blind Bitstream-corrupted Video Recovery: A Visual Foundation Model-driven Framework," in <b>ACM MM 2025</b> (CCF-A)</li>
  <li>&thinsp;C. Cai, T. Liu, J. Gao, W. Liu, <b>K. Wu</b><font color=Blue>*</font>, R. Wang, Y. Wang, S. Liew, "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Grounded Situation Recognition," in <b>ACM MM 2025</b> (Corresponding Author, CCF-A) </li>    
  <li>&thinsp;T. Liu, <b>K. Wu</b><font color=Blue>*</font>, Y. Wang, W. Liu, K. Yap, and L. Chau, â€œBitstream corrupted Video Recovery: A Novel Benchmark Dataset and Method,â€ <b>NeurIPS 2023</b>. (Equal Contribution, CCF-A)</li>
  <li>&thinsp;<b>K. Wu</b>, Z. Li, Y. Yang, Q. Liu, and X. Zhang, â€œEnd-to-end Deep Video Compression Based on Hierarchical Temporal Context Learning,â€ in <b>IEEE Transactions on Multimedia</b>, 2025. (ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;W. Liu, C. Cai, J. Gao, <b>K. Wu</b><font color=Blue>*</font>, Y. Wang, K. Yap, and L. Chau, â€œPromptSR: Cascade Prompting for Lightweight Image Super-Resolution,â€ in <b>IEEE Transactions on Multimedia</b>, 2025. (Corresponding Author, ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;<b>K. Wu</b>, Y. Yang, G. Jiang, and X. Zhang, â€œHierarchical Independent Coding Scheme for Varifocal Multiview Images based on Angular-focal Joint Prediction,â€ <b>IEEE Transactions on Multimedia</b>, 26:2993-3006, 2024. (ä¸­ç§‘é™¢1åŒºTOP) $\color{Tomato} {ESI &thinsp;Highly &thinsp;Cited &thinsp;Papers}$</li>
  <li>&thinsp;W. Liu, <b>K. Wu</b><font color=Blue>*</font>, T. Liu, Y. Wang, K. Yap, and L. Chau, â€œByteNet: Rethinking Multimedia File Fragment Classification through Visual Perspectives,â€ in <b>IEEE Transactions on Multimedia</b>, 2024. (Corresponding Author, ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, and X. Zhang, â€œFocal Stack Image Compression Based on Basis-Quadtree Representation,â€ in <b>IEEE Transactions on Multimedia</b>, 25:3975-3988, 2023. (ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, Y. Yang, and X. Zhang, â€œGaussian-Wiener Representation and Hierarchical Coding Scheme for Focal Stack Images,â€ in <b>IEEE Transactions on Circuits and Systems for Video Technology</b>, 32(2):523-537, 2022. (ä¸­ç§‘é™¢1åŒºTOP)</li>
  <li>&thinsp;<b>K. Wu</b>, Z. Li, Y. Yang, and Q. Liu, â€œDeep Video Compression based on Long-range Temporal Context Learning,â€ in Computer Vision and Image Understanding, 248(2024): 104127.</li>  
  <li>&thinsp;X. Yu, <b>K. Wu</b><font color=Blue>*</font>, Y. Yang, and Q. Liu, â€œWaRENet: A Novel Urban Waterlogging Risk Evaluation Network,â€ in ACM Transactions on Multimedia Computing, Communications, and Applications, 2024, 20(7):1â€“28. (Equal Contribution)</li>
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, K. Yap, and Y. Yang, â€œMultifocal Multiview Imaging and Data Compression based on Angular-Focal-Spatial Representation,â€ in Optics Letters, 2024.</li>
  <li>&thinsp;J. Gao, K. Yap, K. Wu, D. Phan, and K. Garg, "Contextual Human Object Interaction Understanding from Pre-Trained Large Language Model," ICASSP 2024, Seoul, Korea</li>
  <li>&thinsp;C. Cai, R. Zhang, J. Gao, K. Wu, K. Yap, Y. Wang, "Temporal Sentence Grounding with Temporally Global Textual Knowledge," ICME 2024, Niagra Falls, Canada</li>
<!-- æ®µè½æ³¨é‡Š    
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, K. Yap, and Y. Yang, â€œHigh Dimensional Optical Data Varifocal Multiview Imaging, Compression and Evaluation,â€ in Optics Express, 2023.</li>
  <li>&thinsp;<b>K. Wu</b>, Q. Liu, Y. Wang, and Y. Yang, â€œEnd-to-end Varifocal Multiview Images Coding Framework from Data Acquisition End to Vision Application End,â€ in Optics Express, 31(7): 11659-11679, 2023.</li>  
-->
  </ul>
</div>


</div>

 

<!-- æ®µè½æ³¨é‡Š 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**
-->








<!--    # ğŸ† Honors & Awards   -->

<!-- ä¸­æ–‡å— -->
<div class="i18n zh">
<span class='anchor' id='honors-awards'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">è£èª‰å¥–é¡¹</h1>

<li>æ˜¥æ™–æ¯ä¸­å›½ç•™å­¦äººå‘˜åˆ›æ–°åˆ›ä¸šå¤§èµ›ä¼˜èƒœå¥–ï¼ˆæ’ä¸€ï¼Œæ•™è‚²éƒ¨ï¼‰</li>
<li>å…¨å›½åšå£«ååˆ›æ–°åˆ›ä¸šå¤§èµ›é“œå¥–ï¼ˆæ’ä¸€ï¼ŒäººåŠ›èµ„æºå’Œç¤¾ä¼šä¿éšœéƒ¨ï¼‰</li>
<li>ä¸­å›½å›½é™…å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šå¤§èµ›æ¹–åŒ—çœé‡‘å¥–ï¼ˆæ’äºŒï¼Œæ¹–åŒ—çœæ•™è‚²å…ï¼‰</li>
<li>ä¸­å›½å›½é™…å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šå¤§èµ›å…¨å›½é‡‘å¥–ï¼ˆåŸäº’è”ç½‘+ï¼Œæ•™è‚²éƒ¨ï¼‰</li>
<li>IET Excellence and Innovation Awards International Awards (Silver Award)</li>
<li>IET Impact in Society Awards (Shortlist)</li>
<li>Letter of Appreciation from University of Oxford</li>

</div>


<!-- è‹±æ–‡å— -->
<div class="i18n en">
<span class='anchor' id='honors-awards'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">Honors & Awards</h1>

<li>China International College Students' Innovation Competition (formerly Internet +, Gold Award)</li>
<li>IET Excellence and Innovation Awards International Awards (Silver Award)</li>
<li>National Postdoctoral Innovation and Entrepreneurship Competition (Bronze Award, ranked first)</li>
<li>Chunhui Cup Innovation and Entrepreneurship Competition for Chinese Overseas Students (ranked first)</li>
<li>IET Impact in Society Awards (Shortlist)</li>
<li>Letter of Appreciation from University of Oxford</li>

</div>






<!--     # Educations ğŸ“–     -->

<!-- ä¸­æ–‡å— -->
<div class="i18n zh">
<span class='anchor' id='educations'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">æ•™è‚²ç»å†</h1>

<li>è”åˆåŸ¹å…»åšå£«ç ”ç©¶ç”Ÿï¼Œæ–°åŠ å¡å—æ´‹ç†å·¥å¤§å­¦ç”µæ°”ä¸ç”µå­å·¥ç¨‹å­¦é™¢</li>
<li>åšå£«å­¦ä½ï¼Œåä¸­ç§‘æŠ€å¤§å­¦ç”µå­ä¿¡æ¯ä¸é€šä¿¡å­¦é™¢</li>
<li>ç¡•å£«å­¦ä½ï¼Œå“ˆå°”æ»¨å·¥ç¨‹å¤§å­¦æ™ºèƒ½ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li>
<li>å­¦å£«å­¦ä½ï¼Œä¸Šæµ·å¤§å­¦æœºç”µå·¥ç¨‹ä¸è‡ªåŠ¨åŒ–å­¦é™¢</li>

</div>


<!-- è‹±æ–‡å— -->
<div class="i18n en">
<span class='anchor' id='educations'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">Educations</h1>

<li>Exchange Ph.D Student, School of Electrical and Electronic Engineering, Nanyang Technological University</li>
<li>Ph.D Student, School of Electronic Information and Communications, Huazhong University of Science and Technology</li>
<li>Master Student, College of Intelligent Science and Engineering, Harbin Engineering University</li>
<li>Undergraduate Student, School of Mechatronic Engineering and Automation, Shanghai University</li>
 
</div>






<!--    # Students   -->

<!-- ä¸­æ–‡å— -->
<div class="i18n zh">
<span class='anchor' id='students'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">å­¦ç”ŸåŸ¹å…»</h1>

æˆ‘å¾ˆè£å¹¸èƒ½å‚ä¸åˆ°è¿™äº›ä¼˜ç§€å­¦ç”Ÿçš„åŸ¹å…»è¿‡ç¨‹: 

<li>Q. Zhang, Intern Student, KTH Royal Institute of Technology, Sweden (LLMs on multimedia understanding)</li>
<li>J. Liang, Intern Student, Huazhong University of Science and Technology (LLMs on multimedia understanding)</li>
<li>F. Li, Master Student, Huazhong University of Science and Technology (LLMs on Byte-domain understanding)</li>
<li>C. Zhang, Ph.D Student, Huazhong University of Science and Technology (LLMs on signal processing)</li>
<li>R. Wang, Ph.D Student, Harbin Engineering University (LLMs on fine-grained recognition)</li>
<li>S. Wang, Ph.D Student, Harbin Engineering University (LLMs on anomaly detection)</li>
<!-- æ®µè½æ³¨é‡Š - R. Yan, Undergraduate Student, Xiamen University (LLMs on signal analytics)</li>  -->
<li>J. Gao, Ph.D Student, Nanyang Technological University (LLMs on HOI)</li>
<li>W. Liu, Ph.D Student, Nanyang Technological University (Gen-AI on image restoration)</li>
<li>T. Liu, Ph.D Student, Nanyang Technological University (Gen-AI on image restoration)</li>
<li>C. Cai, Ph.D, Shopee, Singapore (LLMs on Open-vocabulary Grounding)</li>

</div>


<!-- è‹±æ–‡å— -->
<div class="i18n en">
<span class='anchor' id='students'></span>
<h1 style="color:#1155A0; font-size:1.5em; font-weight:bold;">Students</h1>

I'm honored to have participated in the supervision of these excellent students: 

<li>Q. Zhang, Intern Student, KTH Royal Institute of Technology, Sweden (LLMs on multimedia understanding)</li>
<li>J. Liang, Intern Student, Huazhong University of Science and Technology (LLMs on multimedia understanding)</li>
<li>F. Li, Master Student, Huazhong University of Science and Technology (LLMs on Byte-domain understanding)</li>
<li>C. Zhang, Ph.D Student, Huazhong University of Science and Technology (LLMs on signal processing)</li>
<li>R. Wang, Ph.D Student, Harbin Engineering University (LLMs on fine-grained recognition)</li>
<li>S. Wang, Ph.D Student, Harbin Engineering University (LLMs on anomaly detection)</li>
<!-- æ®µè½æ³¨é‡Š - R. Yan, Undergraduate Student, Xiamen University (LLMs on signal analytics)</li>  -->
<li>J. Gao, Ph.D Student, Nanyang Technological University (LLMs on HOI)</li>
<li>W. Liu, Ph.D Student, Nanyang Technological University (Gen-AI on image restoration)</li>
<li>T. Liu, Ph.D Student, Nanyang Technological University (Gen-AI on image restoration)</li>
<li>C. Cai, Ph.D, Shopee, Singapore (LLMs on Open-vocabulary Grounding)</li>

 
</div>


 


 
<!-- æ®µè½æ³¨é‡Š 
<span class='anchor' id='invited-talks'></span>
# Talks/Presentations
[//]: # ğŸ’¡ Talks/Presentations
[//]: ğŸ’¬
[//]: - *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
[//]: - *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)
- Annual Conference on Neural Information Processing Systems (NeurIPS 2023), New Orleans, Louisiana, USA
- 2023 Data Compression Conference (DCC), Snowbird, UT, USA
- 2019 Data Compression Conference (DCC), Snowbird, UT, USA
-->


